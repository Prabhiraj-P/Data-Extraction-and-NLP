{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef5LrCSCyHQt"
      },
      "source": [
        "###Data Extraction and NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "id": "Bxv8si8Wx7Y1"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import pandas as pd\n",
        "import requests\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI-HU-2isjJN",
        "outputId": "7ade44fb-ba08-4dd3-a5b5-b6773b1259ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cmudict to\n",
            "[nltk_data]     C:\\Users\\prabh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 244,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('cmudict')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6_ktcVqIWSa",
        "outputId": "fa1cb750-4bfd-46b8-ad3f-f667f269c970"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\prabh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\prabh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 245,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "id": "QJdspAAmGoyx"
      },
      "outputs": [],
      "source": [
        "positive_fname='D:/blackcoffer/positive-words.txt' #file path of positive word list txt\n",
        "negative_fname='D:/blackcoffer/negative-words.txt'  #file path of negetive word list txt\n",
        "stopword1_fname='D:/blackcoffer/StopWords_Generic.txt' #file path of stopword word list txt\n",
        "input_fname='D:/blackcoffer/Input.xlsx - Sheet1.csv'   #input file path .csv file\n",
        "stopword2_fname='D:/blackcoffer/StopWords_GenericLong.txt' #file path of stopword word list txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "id": "7m244YhcEB54"
      },
      "outputs": [],
      "source": [
        "#list  of pronouns used taken from internet\n",
        "pronouns = ['i', 'me', 'my', 'mine', 'you', 'your', 'yours', 'he', 'him', 'his', 'she', 'her', 'hers', 'it', 'its', 'we', 'us', 'our', 'ours', 'they', 'them', 'their', 'theirs']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "id": "cS1D4pPjGy6_"
      },
      "outputs": [],
      "source": [
        "from os import remove\n",
        "positive=[]\n",
        " # Declaring positive words list\n",
        "negative=[] \n",
        "# Declaring negative words list\n",
        "stopword=[]\n",
        " # Declaring stopwords words list\n",
        "\n",
        "with open(positive_fname,mode='r') as file:\n",
        "  positive_words=file.readlines() #opening txt file\n",
        "\n",
        "for word in positive_words:            #adding files to list\n",
        "  positive.append(word[:len(word)-1])\n",
        "\n",
        "with open(negative_fname,mode='r',encoding='ISO-8859-1') as file:\n",
        "  negative_words=file.readlines()\n",
        "\n",
        "for word in negative_words:\n",
        "  negative.append(word[:len(word)-1])     #adding files to list\n",
        "#opening stop words \n",
        "with open(stopword1_fname,mode='r',encoding='ISO-8859-1') as file1,open(stopword2_fname,mode='r',encoding='ISO-8859-1')as file2:\n",
        "  stopword_words=file1.readlines()\n",
        "  stopword_words2=file2.readlines()\n",
        "for word in stopword_words+stopword_words2:\n",
        "  stopword.append(word[:len(word)-1])     #adding files to list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {},
      "outputs": [],
      "source": [
        "#removing pronouns from stopwords\n",
        "#we need the count of pronouns from article \n",
        "for i in pronouns:\n",
        "    if i in stopword: \n",
        "        stopword.remove(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize    \n",
        "import nltk  #nltk "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_df=pd.read_csv(input_fname) #reading csv Dataframe input_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {},
      "outputs": [],
      "source": [
        "#list to remove unwanted lines from article. repeating sendences\n",
        "waste=[\"Output exceeds the size limit. Open the full output data in a text editor1\",\"Introduction\",\"Contact us: hello@blackcoffer.com\",\"Â© All Right Reserved, Blackcoffer(OPC) Pvt. Ltd\",\"Ranking customer behaviours for business strategy\",\"Algorithmic trading for multiple commodities markets, like Forex, Metals, Energy, etc.\",\"Trading Bot for FOREX\",\"Python model for the analysis of sector-specific stock ETFs for investment purposes\",\"Playstore & Appstore to Google Analytics (GA) or Firebase to Google Data Studio Mobile App KPI Dashboard\",\"Google Local Service Ads LSA API To Google BigQuery to Google Data Studio\",\"AI Conversational Bot using RASA\",\"Recommendation System Architecture\",\"Rise of telemedicine and its Impact on Livelihood by 2040\",\"Rise of e-health and its impact on humans by the year 2030\",\"Rise of e-health and its impact on humans by the year 2030\",\"Rise of telemedicine and its Impact on Livelihood by 2040\",\"AI/ML and Predictive Modeling\",\"Solution for Contact Centre Problems\",\"How to Setup Custom Domain for Google App Engine Application?\",\"Code Review Checklist\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Code to web scrap from given links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6dwBfYSzVPX",
        "outputId": "4acbf26d-3cd4-415c-9be7-4392803d0098"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/114 of file completed\n",
            "2/114 of file completed\n",
            "3/114 of file completed\n",
            "4/114 of file completed\n",
            "5/114 of file completed\n",
            "6/114 of file completed\n",
            "7/114 of file completed\n",
            "8/114 of file completed\n",
            "9/114 of file completed\n",
            "10/114 of file completed\n",
            "11/114 of file completed\n",
            "12/114 of file completed\n",
            "13/114 of file completed\n",
            "14/114 of file completed\n",
            "15/114 of file completed\n",
            "16/114 of file completed\n",
            "17/114 of file completed\n",
            "18/114 of file completed\n",
            "19/114 of file completed\n",
            "20/114 of file completed\n",
            "21/114 of file completed\n",
            "22/114 of file completed\n",
            "23/114 of file completed\n",
            "24/114 of file completed\n",
            "25/114 of file completed\n",
            "26/114 of file completed\n",
            "27/114 of file completed\n",
            "28/114 of file completed\n",
            "29/114 of file completed\n",
            "30/114 of file completed\n",
            "31/114 of file completed\n",
            "32/114 of file completed\n",
            "33/114 of file completed\n",
            "34/114 of file completed\n",
            "35/114 of file completed\n",
            "36/114 of file completed\n",
            "37/114 of file completed\n",
            "38/114 of file completed\n",
            "39/114 of file completed\n",
            "40/114 of file completed\n",
            "41/114 of file completed\n",
            "42/114 of file completed\n",
            "43/114 of file completed\n",
            "44/114 of file completed\n",
            "45/114 of file completed\n",
            "46/114 of file completed\n",
            "47/114 of file completed\n",
            "48/114 of file completed\n",
            "49/114 of file completed\n",
            "50/114 of file completed\n",
            "51/114 of file completed\n",
            "52/114 of file completed\n",
            "53/114 of file completed\n",
            "54/114 of file completed\n",
            "55/114 of file completed\n",
            "56/114 of file completed\n",
            "57/114 of file completed\n",
            "58/114 of file completed\n",
            "59/114 of file completed\n",
            "60/114 of file completed\n",
            "61/114 of file completed\n",
            "62/114 of file completed\n",
            "63/114 of file completed\n",
            "64/114 of file completed\n",
            "65/114 of file completed\n",
            "66/114 of file completed\n",
            "67/114 of file completed\n",
            "68/114 of file completed\n",
            "69/114 of file completed\n",
            "70/114 of file completed\n",
            "71/114 of file completed\n",
            "72/114 of file completed\n",
            "73/114 of file completed\n",
            "74/114 of file completed\n",
            "75/114 of file completed\n",
            "76/114 of file completed\n",
            "77/114 of file completed\n",
            "78/114 of file completed\n",
            "79/114 of file completed\n",
            "80/114 of file completed\n",
            "81/114 of file completed\n",
            "82/114 of file completed\n",
            "83/114 of file completed\n",
            "84/114 of file completed\n",
            "85/114 of file completed\n",
            "86/114 of file completed\n",
            "87/114 of file completed\n",
            "88/114 of file completed\n",
            "89/114 of file completed\n",
            "90/114 of file completed\n",
            "91/114 of file completed\n",
            "92/114 of file completed\n",
            "93/114 of file completed\n",
            "94/114 of file completed\n",
            "95/114 of file completed\n",
            "96/114 of file completed\n",
            "97/114 of file completed\n",
            "98/114 of file completed\n",
            "99/114 of file completed\n",
            "100/114 of file completed\n",
            "101/114 of file completed\n",
            "102/114 of file completed\n",
            "103/114 of file completed\n",
            "104/114 of file completed\n",
            "105/114 of file completed\n",
            "106/114 of file completed\n",
            "107/114 of file completed\n",
            "108/114 of file completed\n",
            "109/114 of file completed\n",
            "110/114 of file completed\n",
            "111/114 of file completed\n",
            "112/114 of file completed\n",
            "113/114 of file completed\n",
            "114/114 of file completed\n",
           ]
        }
      ],
      "source": [
        "from pandas.io.formats.format import format_array\n",
        "i=0\n",
        "main_text=[]  # declating main list to add up all article\n",
        "for url in input_df['URL']: # to loop through all links in input\n",
        " i+=1                          # To know which linkis currently scrapping\n",
        " text=\"\"                    # declaring text to nill\n",
        " response=requests.get(url)\n",
        " soup=BeautifulSoup(response.content,'html.parser')\n",
        " main_div=soup.find_all('div',{ 'class':\"tdb-block-inner td-fix-index\"})\n",
        " text2=soup.find_all('p')  #to find all p tag\n",
        " print(\"{}/{} of file completed\".format(i,len(input_df))) \n",
        " for div in text2:\n",
        "   if div.text not in waste:\n",
        "    text=text+div.text     #add all lines of  article to one into one string\n",
        " main_text.append([text])       # Append text to main_rext\n",
        "#print(main_text)     \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "114"
            ]
          },
          "execution_count": 254,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(main_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#importing countVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {},
      "outputs": [],
      "source": [
        "#creating object of count vectoriser\n",
        "vectoriser=CountVectorizer(lowercase=True,tokenizer=word_tokenize,stop_words=stopword,max_features=12440)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {},
      "outputs": [],
      "source": [
        "#making it into list of string\n",
        "input_words=[]\n",
        "for article in main_text:\n",
        "    input_words.append(article[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\prabh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "C:\\Users\\prabh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'ai', 'ca', 'he', 'her', 'hers', 'him', 'his', 'i', 'it', 'its', 'me', 'my', \"n't\", 'our', 'ours', 'she', 'their', 'theirs', 'them', 'they', 'we', 'wo', 'you', 'your', 'yours', 'yourselve'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(max_features=12440,\n",
              "                stop_words=[&#x27;ABOUT&#x27;, &#x27;ABOVE&#x27;, &#x27;AFTER&#x27;, &#x27;AGAIN&#x27;, &#x27;ALL&#x27;, &#x27;AM&#x27;,\n",
              "                            &#x27;AMONG&#x27;, &#x27;AN&#x27;, &#x27;AND&#x27;, &#x27;ANY&#x27;, &#x27;ARE&#x27;, &#x27;AS&#x27;, &#x27;AT&#x27;,\n",
              "                            &#x27;BE&#x27;, &#x27;BECAUSE&#x27;, &#x27;BEEN&#x27;, &#x27;BEFORE&#x27;, &#x27;BEING&#x27;, &#x27;BELOW&#x27;,\n",
              "                            &#x27;BETWEEN&#x27;, &#x27;BOTH&#x27;, &#x27;BUT&#x27;, &#x27;BY&#x27;, &#x27;CAN&#x27;, &#x27;DID&#x27;, &#x27;DO&#x27;,\n",
              "                            &#x27;DOES&#x27;, &#x27;DOING&#x27;, &#x27;DOWN&#x27;, &#x27;DURING&#x27;, ...],\n",
              "                tokenizer=&lt;function word_tokenize at 0x0000022C124BA200&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_features=12440,\n",
              "                stop_words=[&#x27;ABOUT&#x27;, &#x27;ABOVE&#x27;, &#x27;AFTER&#x27;, &#x27;AGAIN&#x27;, &#x27;ALL&#x27;, &#x27;AM&#x27;,\n",
              "                            &#x27;AMONG&#x27;, &#x27;AN&#x27;, &#x27;AND&#x27;, &#x27;ANY&#x27;, &#x27;ARE&#x27;, &#x27;AS&#x27;, &#x27;AT&#x27;,\n",
              "                            &#x27;BE&#x27;, &#x27;BECAUSE&#x27;, &#x27;BEEN&#x27;, &#x27;BEFORE&#x27;, &#x27;BEING&#x27;, &#x27;BELOW&#x27;,\n",
              "                            &#x27;BETWEEN&#x27;, &#x27;BOTH&#x27;, &#x27;BUT&#x27;, &#x27;BY&#x27;, &#x27;CAN&#x27;, &#x27;DID&#x27;, &#x27;DO&#x27;,\n",
              "                            &#x27;DOES&#x27;, &#x27;DOING&#x27;, &#x27;DOWN&#x27;, &#x27;DURING&#x27;, ...],\n",
              "                tokenizer=&lt;function word_tokenize at 0x0000022C124BA200&gt;)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "CountVectorizer(max_features=12440,\n",
              "                stop_words=['ABOUT', 'ABOVE', 'AFTER', 'AGAIN', 'ALL', 'AM',\n",
              "                            'AMONG', 'AN', 'AND', 'ANY', 'ARE', 'AS', 'AT',\n",
              "                            'BE', 'BECAUSE', 'BEEN', 'BEFORE', 'BEING', 'BELOW',\n",
              "                            'BETWEEN', 'BOTH', 'BUT', 'BY', 'CAN', 'DID', 'DO',\n",
              "                            'DOES', 'DOING', 'DOWN', 'DURING', ...],\n",
              "                tokenizer=<function word_tokenize at 0x0000022C124BA200>)"
            ]
          },
          "execution_count": 258,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectoriser.fit(input_words) #creating words list bag of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {},
      "outputs": [],
      "source": [
        "input=vectoriser.fit_transform(input_words)  #transforming atricle into bag of method vector form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['!', '#', '$', ..., 'â¹4,200', 'â¹419', 'â¹59'], dtype=object)"
            ]
          },
          "execution_count": 261,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectoriser.get_feature_names_out() #to get list of words in bag of method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# url='https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/' \n",
        "# response=requests.get(url)\n",
        "# soup=BeautifulSoup(response.content,'html.parser')\n",
        "# main_div=soup.find_all('div',{ 'class':\"tdb-block-inner td-fix-index\"})    #train code\n",
        "# text2=soup.find_all('p')\n",
        "# sec_text=[]\n",
        "# print(i)\n",
        "# for div in text2:\n",
        "#    if div.text not in waste:\n",
        "#     text.append(div.text)\n",
        "# print(text) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {},
      "outputs": [],
      "source": [
        "#saved all unique words to unique_words\n",
        "unique_words=vectoriser.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {},
      "outputs": [],
      "source": [
        "#assigning everything with zero\n",
        "POSITIVE_SCORE=0 #\n",
        "NEGATIVE_SCORE=0 #\n",
        "POLARITY_SCORE=0 #\n",
        "SUBJECTIVITY_SCORE=0 #\n",
        "AVG_SENTENCE_LENGTH=0#\n",
        "PERCENTAGE_OF_COMPLEX_WORDS=0 #\n",
        "FOG_INDEX=0 #\n",
        "AVG_NUMBER_OF_WORDS_PER_SENTENCE=0 #\n",
        "COMPLEX_WORD_COUNT=0 #\n",
        "WORD_COUNT=0 #\n",
        "SYLLABLE_PER_WORD=0 #\n",
        "PERSONAL_PRONOUNS=0 #\n",
        "AVG_WORD_LENGTH=0 #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {},
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "def is_complex(word):\n",
        "    # Strip any punctuation from the word\n",
        "    word = word.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Count the number of vowels in the word\n",
        "    num_vowels = len(re.findall(r'[aeiouy]+', word, re.IGNORECASE))\n",
        "\n",
        "    # Count the number of syllables based on the number of vowels\n",
        "    num_syllables = max(1, num_vowels - 1)\n",
        "\n",
        "    # Determine whether the word is complex based on its number of syllables\n",
        "    if num_syllables >= 3:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {},
      "outputs": [],
      "source": [
        "def syll(word):\n",
        " i=0\n",
        " for char in word:\n",
        "    if char in ['a,','e','i','o','u']:\n",
        "      i+=1\n",
        " return i          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
            ]
          },
          "execution_count": 268,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input.toarray()[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_array=input.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 3, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
            ]
          },
          "execution_count": 270,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {},
      "outputs": [],
      "source": [
        "output=[]\n",
        "for value in input.toarray():\n",
        "    syll_value = 0\n",
        "    word_len = 0\n",
        "    no_complex = 0\n",
        "    POSITIVE_SCORE = 0\n",
        "    NEGATIVE_SCORE = 0\n",
        "    PERSONAL_PRONOUNS = 0\n",
        "    sum_value_second = sum(value)\n",
        "    \n",
        "    for i, value_second in enumerate(value):\n",
        "        if value_second > 0:\n",
        "            vector_index = i\n",
        "            value = unique_words[vector_index]\n",
        "            syll_value += syll(value)\n",
        "            word_len += len(value)\n",
        "            if value in pronouns:      \n",
        "                 PERSONAL_PRONOUNS+= 1\n",
        "            if syll_value >= 3:\n",
        "                if is_complex(value):\n",
        "                    no_complex += 1\n",
        "                \n",
        "            if value in positive:\n",
        "                POSITIVE_SCORE += value_second\n",
        "            elif value in negative:\n",
        "                NEGATIVE_SCORE += value_second\n",
        "            \n",
        "                \n",
        "    POLARITY_SCORE = ((POSITIVE_SCORE - NEGATIVE_SCORE) / (POSITIVE_SCORE + NEGATIVE_SCORE + 0.000001))\n",
        "    SUBJECTIVITY_SCORE = ((POSITIVE_SCORE + NEGATIVE_SCORE) / (sum_value_second + 0.000001))\n",
        "    dot_index = vectoriser.vocabulary_.get(\".\")\n",
        "    AVG_SENTENCE_LENGTH = sum_value_second / (input[:, dot_index].sum() + 0.000001)\n",
        "    PERCENTAGE_OF_COMPLEX_WORDS = no_complex / (sum_value_second + 0.000001)\n",
        "    FOG_INDEX = 0.4 * ((AVG_SENTENCE_LENGTH) + (PERCENTAGE_OF_COMPLEX_WORDS * 100))\n",
        "    AVG_NUMBER_OF_WORDS_PER_SENTENCE = sum_value_second / (input[:, dot_index].sum() + 0.000001)\n",
        "    COMPLEX_WORD_COUNT = no_complex\n",
        "    WORD_COUNT = sum_value_second\n",
        "    SYLLABLE_PER_WORD = syll_value / (sum_value_second + 0.000001)\n",
        "    AVG_WORD_LENGTH = word_len / (sum_value_second + 0.000001)\n",
        "    output.append([POSITIVE_SCORE,NEGATIVE_SCORE,POLARITY_SCORE,SUBJECTIVITY_SCORE,AVG_SENTENCE_LENGTH,PERCENTAGE_OF_COMPLEX_WORDS,FOG_INDEX,AVG_NUMBER_OF_WORDS_PER_SENTENCE,COMPLEX_WORD_COUNT,WORD_COUNT,SYLLABLE_PER_WORD,PERSONAL_PRONOUNS,AVG_WORD_LENGTH])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_list=input_df.values.tolist() #converting input dataframe to list "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_list_df=pd.DataFrame(input_list,columns=[\"URL_ID\",\"URL\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in  range(0,len(input_list)):\n",
        "    output[i]=input_list[i]+output[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {},
      "outputs": [],
      "source": [
        "#converting output to dataframre\n",
        "output_df=pd.DataFrame(output,columns=['Url_id','url','POSITIVE_SCORE','NEGATIVE_SCORE','POLARITY_SCORE','SUBJECTIVITY_SCORE','AVG_SENTENCE_LENGTH','PERCENTAGE_OF_COMPLEX_WORDS','FOG_INDEX','AVG_NUMBER_OF_WORDS_PER_SENTENCE','COMPLEX_WORD_COUNT','WORD_COUNT','SYLLABLE_PER_WORD','PERSONAL_PRONOUNS','AVG_WORD_LENGTH'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_df #final output "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "metadata": {},
      "outputs": [],
      "source": [
        "#converting output to csv\n",
        "output_df.to_csv('D:\\\\blackcoffer\\\\submission.csv',index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
