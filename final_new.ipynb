{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef5LrCSCyHQt"
      },
      "source": [
        "###Data Extraction and NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Bxv8si8Wx7Y1"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import pandas as pd\n",
        "import requests\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI-HU-2isjJN",
        "outputId": "7ade44fb-ba08-4dd3-a5b5-b6773b1259ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cmudict to\n",
            "[nltk_data]     C:\\Users\\prabh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('cmudict')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6_ktcVqIWSa",
        "outputId": "fa1cb750-4bfd-46b8-ad3f-f667f269c970"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\prabh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\prabh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QJdspAAmGoyx"
      },
      "outputs": [],
      "source": [
        "positive_fname='D:/blackcoffer/positive-words.txt'\n",
        "negative_fname='D:/blackcoffer/negative-words.txt'\n",
        "stopword1_fname='D:/blackcoffer/StopWords_Generic.txt'\n",
        "input_fname='D:/blackcoffer/Input.xlsx - Sheet1.csv'\n",
        "stopword2_fname='D:/blackcoffer/StopWords_GenericLong.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7m244YhcEB54"
      },
      "outputs": [],
      "source": [
        "pronouns = ['i', 'me', 'my', 'mine', 'you', 'your', 'yours', 'he', 'him', 'his', 'she', 'her', 'hers', 'it', 'its', 'we', 'us', 'our', 'ours', 'they', 'them', 'their', 'theirs']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "cS1D4pPjGy6_"
      },
      "outputs": [],
      "source": [
        "from os import remove\n",
        "positive=[]\n",
        "negative=[]\n",
        "stopword=[]\n",
        "with open(positive_fname,mode='r') as file:\n",
        "  positive_words=file.readlines()\n",
        "\n",
        "for word in positive_words:\n",
        "  positive.append(word[:len(word)-1])\n",
        "\n",
        "with open(negative_fname,mode='r',encoding='ISO-8859-1') as file:\n",
        "  negative_words=file.readlines()\n",
        "\n",
        "for word in negative_words:\n",
        "  negative.append(word[:len(word)-1])\n",
        "#opening stop words\n",
        "with open(stopword1_fname,mode='r',encoding='ISO-8859-1') as file1,open(stopword2_fname,mode='r',encoding='ISO-8859-1')as file2:\n",
        "  stopword_words=file1.readlines()\n",
        "  stopword_words2=file2.readlines()\n",
        "for word in stopword_words+stopword_words2:\n",
        "  stopword.append(word[:len(word)-1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "692"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(stopword)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl35bGufGg7u"
      },
      "source": [
        "code to read positive and negative words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoR3Vc2sf6D0"
      },
      "source": [
        "###Parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "owcIyNh9DXNt"
      },
      "outputs": [],
      "source": [
        "def calculate_syllables_per_word(text):\n",
        "    cmudict = nltk.corpus.cmudict.dict()\n",
        "    complex_count = 0\n",
        "    total_syllables = 0\n",
        "    total_words = 0\n",
        "    for word in text:\n",
        "        syllables = cmudict.get(word.lower())\n",
        "        if syllables:\n",
        "            num_syllables = len(list(filter(lambda s: s[-1].isdigit(), syllables[0])))\n",
        "        else:\n",
        "            num_syllables = 0\n",
        "        total_syllables += num_syllables\n",
        "        complex_count += 1\n",
        "        total_words += 1\n",
        "    return total_syllables / total_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize    \n",
        "import nltk "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['hello', 'my', 'dear']\n"
          ]
        }
      ],
      "source": [
        "print(word_tokenize(\"hello my dear\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YZgCH54InJT3"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "def subj_score(input):\n",
        " sia=SentimentIntensityAnalyzer()\n",
        " scores=sia.polarity_scores(\" \".join(input))\n",
        " subjectivity_score = scores['compound']\n",
        " return subj_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "#vector.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVFhbzx3mUc1"
      },
      "source": [
        "###void main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLsqemNHmUVi"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IuZK9zptnFOn"
      },
      "outputs": [],
      "source": [
        "#df=pd.DataFrame(main_list,columns=['POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE',\"SUBJECTIVITY SCORE\",'AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH'])\n",
        "#df.to_csv('1.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_df=pd.read_csv(input_fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "waste=[\"Output exceeds the size limit. Open the full output data in a text editor1\",\"Introduction\",\"Contact us: hello@blackcoffer.com\",\"© All Right Reserved, Blackcoffer(OPC) Pvt. Ltd\",\"Ranking customer behaviours for business strategy\",\"Algorithmic trading for multiple commodities markets, like Forex, Metals, Energy, etc.\",\"Trading Bot for FOREX\",\"Python model for the analysis of sector-specific stock ETFs for investment purposes\",\"Playstore & Appstore to Google Analytics (GA) or Firebase to Google Data Studio Mobile App KPI Dashboard\",\"Google Local Service Ads LSA API To Google BigQuery to Google Data Studio\",\"AI Conversational Bot using RASA\",\"Recommendation System Architecture\",\"Rise of telemedicine and its Impact on Livelihood by 2040\",\"Rise of e-health and its impact on humans by the year 2030\",\"Rise of e-health and its impact on humans by the year 2030\",\"Rise of telemedicine and its Impact on Livelihood by 2040\",\"AI/ML and Predictive Modeling\",\"Solution for Contact Centre Problems\",\"How to Setup Custom Domain for Google App Engine Application?\",\"Code Review Checklist\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6dwBfYSzVPX",
        "outputId": "4acbf26d-3cd4-415c-9be7-4392803d0098"
      },
      "outputs": [],
      "source": [
        "from pandas.io.formats.format import format_array\n",
        "i=0\n",
        "main_text=[]\n",
        "for url in input_df['URL']:\n",
        " i+=1\n",
        " text=\"\"\n",
        " response=requests.get(url)\n",
        " soup=BeautifulSoup(response.content,'html.parser')\n",
        " main_div=soup.find_all('div',{ 'class':\"tdb-block-inner td-fix-index\"})\n",
        " text2=soup.find_all('p')\n",
        " for div in text2:\n",
        "   if div.text not in waste:\n",
        "    text=text+div.text\n",
        " print(i)\n",
        " main_text.append([text])\n",
        "print(main_text)      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "114"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(main_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#word_tokenize(''.join(main_text[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "vectoriser=CountVectorizer(lowercase=True,tokenizer=word_tokenize,stop_words=stopword,max_features=100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_text=''\n",
        "for a in main_text:\n",
        "    for b in a[0]:\n",
        "        vect_text=vect_text+b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_words=[]\n",
        "for article in main_text:\n",
        "    input_words.append(article[0])\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "114"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(input_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "vectoriser.fit(input_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\prabh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "input=vectoriser.fit_transform(input_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "a=input.toarray()[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['!', '#', '$', ..., '₹419', '₹59', '≈50'], dtype=object)"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectoriser.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12440"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "url='https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/' \n",
        "response=requests.get(url)\n",
        "soup=BeautifulSoup(response.content,'html.parser')\n",
        "main_div=soup.find_all('div',{ 'class':\"tdb-block-inner td-fix-index\"})\n",
        "text2=soup.find_all('p')\n",
        "sec_text=[]\n",
        "print(i)\n",
        "for div in text2:\n",
        "   if div.text not in waste:\n",
        "    text.append(div.text)\n",
        "print(text) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "#saved all unique words to unique_words\n",
        "unique_words=vectoriser.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [],
      "source": [
        "#assigning everything with zero\n",
        "POSITIVE_SCORE=0 #\n",
        "NEGATIVE_SCORE=0 #\n",
        "POLARITY_SCORE=0 #\n",
        "SUBJECTIVITY_SCORE=0 #\n",
        "AVG_SENTENCE_LENGTH=0#\n",
        "PERCENTAGE_OF_COMPLEX_WORDS=0 #\n",
        "FOG_INDEX=0 #\n",
        "AVG_NUMBER_OF_WORDS_PER_SENTENCE=0 #\n",
        "COMPLEX_WORD_COUNT=0 #\n",
        "WORD_COUNT=0 #\n",
        "SYLLABLE_PER_WORD=0 #\n",
        "PERSONAL_PRONOUNS=0 #\n",
        "AVG_WORD_LENGTH=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "def is_complex(word):\n",
        "    # Strip any punctuation from the word\n",
        "    word = word.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Count the number of vowels in the word\n",
        "    num_vowels = len(re.findall(r'[aeiouy]+', word, re.IGNORECASE))\n",
        "\n",
        "    # Count the number of syllables based on the number of vowels\n",
        "    num_syllables = max(1, num_vowels - 1)\n",
        "\n",
        "    # Determine whether the word is complex based on its number of syllables\n",
        "    if num_syllables >= 3:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "def syll(word):\n",
        " i=0\n",
        " for char in word:\n",
        "    if char in ['a,','e','i','o','u']:\n",
        "      i+=1\n",
        " return i          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
            ]
          },
          "execution_count": 156,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input.toarray()[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_array=input.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 3, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [],
      "source": [
        "for value in input.toarray()[:1]:\n",
        "    syll_value = 0\n",
        "    word_len = 0\n",
        "    no_complex = 0\n",
        "    POSITIVE_SCORE = 0\n",
        "    NEGATIVE_SCORE = 0\n",
        "    PERSONAL_PRONOUNS = 0\n",
        "    sum_value_second = sum(value)\n",
        "    \n",
        "    for i, value_second in enumerate(value):\n",
        "        if value_second > 0:\n",
        "            vector_index = i\n",
        "            value = unique_words[vector_index]\n",
        "            syll_value += syll(value)\n",
        "            word_len += len(value)\n",
        "            \n",
        "            if syll_value >= 3:\n",
        "                if is_complex(value):\n",
        "                    no_complex += 1\n",
        "                \n",
        "            if value in positive:\n",
        "                POSITIVE_SCORE += value_second\n",
        "            elif value in negative:\n",
        "                NEGATIVE_SCORE += value_second\n",
        "            elif value in pronouns:\n",
        "                PERSONAL_PRONOUNS += 1\n",
        "                \n",
        "    POLARITY_SCORE = ((POSITIVE_SCORE - NEGATIVE_SCORE) / (POSITIVE_SCORE + NEGATIVE_SCORE + 0.000001))\n",
        "    SUBJECTIVITY_SCORE = ((POSITIVE_SCORE + NEGATIVE_SCORE) / (sum_value_second + 0.000001))\n",
        "    dot_index = vectoriser.vocabulary_.get(\".\")\n",
        "    AVG_SENTENCE_LENGTH = sum_value_second / (input[:, dot_index].sum() + 0.000001)\n",
        "    PERCENTAGE_OF_COMPLEX_WORDS = no_complex / (sum_value_second + 0.000001)\n",
        "    FOG_INDEX = 0.4 * ((AVG_SENTENCE_LENGTH) + (PERCENTAGE_OF_COMPLEX_WORDS * 100))\n",
        "    AVG_NUMBER_OF_WORDS_PER_SENTENCE = sum_value_second / (input[:, dot_index].sum() + 0.000001)\n",
        "    COMPLEX_WORD_COUNT = no_complex\n",
        "    WORD_COUNT = sum_value_second\n",
        "    SYLLABLE_PER_WORD = syll_value / (sum_value_second + 0.000001)\n",
        "    AVG_WORD_LENGTH = word_len / (sum_value_second + 0.000001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.515151511453602\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'method' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[162], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mtoarray[:\u001b[39m1\u001b[39;49m]: \n\u001b[0;32m      2\u001b[0m     syll_value\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m      3\u001b[0m     word_len\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m                  \n",
            "\u001b[1;31mTypeError\u001b[0m: 'method' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "\n",
        "for value in input.toarray[:1]: \n",
        "    syll_value=0\n",
        "    word_len=0                  \n",
        "         #sort one list from all  list\n",
        "    for value_second in value:          \n",
        "          #sort first number from one list\n",
        "     if value_second>0:                \n",
        "            #check if value>0(if word exist)\n",
        "        vector_intex=value.index(value_second) \n",
        "               #if yes return the intex of value\n",
        "        value=unique_words[vector_intex]    \n",
        "           #from index get the word from unique word list and assigning it to value\n",
        "        syll_value+=syll(value)\n",
        "                         #increamenting number or sylabell in word\n",
        "        word_len+=len(value)\n",
        "                         #calculate length of each word\n",
        "        if syll_value>=3:\n",
        "                   #only checking if conplex if  no of syllal value greater than 3 or equal three\n",
        "         if is_complex(value):\n",
        "           no_complex=+1\n",
        "        if value in positive:               \n",
        "               #cheking value in positive or negative\n",
        "            POSITIVE_SCORE+=value_second     \n",
        "               #if it is in positive or negetive adding the count\n",
        "        elif value in negative:\n",
        "            NEGATIVE_SCORE+=value_second\n",
        "            \n",
        "        elif value in pronouns:\n",
        "           PERSONAL_PRONOUNS+=1\n",
        "                \n",
        "    POLARITY_SCORE=((POSITIVE_SCORE-NEGATIVE_SCORE)/(POSITIVE_SCORE+NEGATIVE_SCORE))+0.000001\n",
        "    \n",
        "    SUBJECTIVITY_SCORE=((POSITIVE_SCORE+NEGATIVE_SCORE)/sum(value_second))+0.000001\n",
        "    dot_intex=vectoriser.vocabulary_.get(\".\") \n",
        "                       #taking index of \".\" to calculate number of occarance of \".\"\n",
        "    AVG_SENTENCE_LENGTH=sum(value_second)/(input[:, dot_intex].sum()) \n",
        "                       #number of \".\" gives number of sendences=number of words/nummber of sendence\n",
        "    PERCENTAGE_OF_COMPLEX_WORDS=no_complex/(input[:, dot_intex].sum())                              \n",
        "                   #nummber of complex words /number of sendence  \n",
        "    FOG_INDEX=0.4*((sum(value_second)/(input[:, dot_intex].sum()) )+PERCENTAGE_OF_COMPLEX_WORDS)\n",
        "                   #= 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
        "    AVG_NUMBER_OF_WORDS_PER_SENTENCE=sum(value_second)/(input[:, dot_intex].sum())\n",
        "                                 #the total number of words / the total number of sentences\n",
        "    COMPLEX_WORD_COUNT=no_complex \n",
        "                          #directly from above\n",
        "    WORD_COUNT=sum(value_second)\n",
        "    SYLLABLE_PER_WORD=syll_value/sum(value_second)        #no of syllabel/ number of words\n",
        "    AVG_WORD_LENGTH=word_len/sum(value_second)\n",
        "                   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 149,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "AVG_WORD_LENGTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
